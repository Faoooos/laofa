{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPm0ipfdyIEazlDfGAhZQ6+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"endhTr_e_hH6"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import math\n","from transformers import pipeline\n","from sentence_transformers import SentenceTransformer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","import re\n","from scipy import stats\n","from sklearn.preprocessing import MinMaxScaler\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import networkx as nx\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.metrics.pairwise import rbf_kernel\n","from torch_geometric.utils import dense_to_sparse\n","from torch_geometric.nn import TAGConv\n","from torch_geometric.nn import GCNConv\n","from torch_geometric.nn import GATConv\n","from sklearn.metrics import classification_report, roc_auc_score, average_precision_score, f1_score"]},{"cell_type":"code","source":["\n","\n","UNIT_CONVERSION = {\n","\n","    'weight': {\n","        'kg': 1.0,\n","        'g': 0.001,\n","        'mg': 0.000001,\n","        'lb': 0.453592,\n","        'oz': 0.0283495\n","    },\n","\n","    'length': {\n","        'm': 1.0,\n","        'cm': 0.01,\n","        'mm': 0.001,\n","        'km': 1000.0,\n","        'in': 0.0254,\n","        'ft': 0.3048,\n","        'mi': 1609.34\n","    },\n","\n","    'time': {\n","        's': 1.0,\n","        'ms': 0.001,\n","        'min': 60.0,\n","        'h': 3600.0,\n","        'day': 86400.0\n","    },\n","\n","    'volume': {\n","        'l': 1.0,\n","        'ml': 0.001,\n","        'm3': 1000.0,\n","        'gal': 3.78541,\n","        'qt': 0.946353,\n","        'pt': 0.473176\n","    }\n","}\n","\n","UNIT_CATEGORIES = {\n","\n","    'kg': 'weight', 'g': 'weight', 'mg': 'weight', 'lb': 'weight', 'oz': 'weight',\n","    'm': 'length', 'cm': 'length', 'mm': 'length', 'km': 'length', 'in': 'length',\n","    'ft': 'length', 'mi': 'length',\n","    's': 'time', 'ms': 'time', 'min': 'time', 'h': 'time', 'day': 'time',\n","    'l': 'volume', 'ml': 'volume', 'm3': 'volume', 'gal': 'volume', 'qt': 'volume', 'pt': 'volume'\n","\n","}\n","\n","\n","\n","def extract_unit_from_column(col_name):\n","\n","    col_name = str(col_name).lower()\n","    underscore_match = re.search(r'_([a-z]{1,4})$', col_name)\n","    if underscore_match:\n","        return underscore_match.group(1)\n","    bracket_match = re.search(r'\\(([a-z]{1,4})\\)$', col_name)\n","    if bracket_match:\n","        return bracket_match.group(1)\n","    space_match = re.search(r'\\s([a-z]{1,4})$', col_name)\n","    if space_match:\n","        return space_match.group(1)\n","\n","    return None\n","\n","\n","def detect_unit_category(unit):\n","\n","    return UNIT_CATEGORIES.get(unit.lower(), None)\n","\n","\n","def are_units_convertible(unit1, unit2):\n","\n","    category1 = detect_unit_category(unit1)\n","    category2 = detect_unit_category(unit2)\n","    return category1 is not None and category1 == category2\n","\n","\n","def convert_units(value, from_unit, to_unit):\n","\n","    category = detect_unit_category(from_unit)\n","    if category is None or not are_units_convertible(from_unit, to_unit):\n","        return value\n","\n","    factor_from = UNIT_CONVERSION[category].get(from_unit.lower(), 1.0)\n","    factor_to = UNIT_CONVERSION[category].get(to_unit.lower(), 1.0)\n","\n","    return value * (factor_from / factor_to)\n","\n","\n","def statistical_unit_check(series1, series2):\n","\n","    s1 = series1.dropna()\n","\n","    s2 = series2.dropna()\n","\n","    if len(s1) < 10 or len(s2) < 10:\n","        return False, 1.0\n","    ratio = (s2.mean() / s1.mean()) if s1.mean() != 0 else 1.0\n","    min_length = min(len(s1), len(s2))\n","    s1 = s1.iloc[:min_length]\n","    s2 = s1.iloc[:min_length]\n","    slope, intercept, r_value, p_value, std_err = stats.linregress(s1, s2)\n","    if r_value > 0.9 and abs(intercept) < 0.1 * max(abs(s2.mean()), abs(s1.mean())):\n","        return True, slope\n","    return False, 1.0\n","\n","\n","def find_most_similar_time_col(tab_col, time_cols):\n","\n","    processed_tab_col = preprocess_column_name(tab_col)\n","    processed_time_cols = [preprocess_column_name(col) for col in time_cols]\n","    tab_embedding = model.encode([processed_tab_col])\n","    time_embeddings = model.encode(processed_time_cols)\n","    similarities = cosine_similarity(tab_embedding, time_embeddings)[0]\n","    most_similar_idx = np.argmax(similarities)\n","\n","    return time_cols[most_similar_idx], similarities[most_similar_idx]\n","\n","\n","def preprocess_data(*dfs):\n","\n","    scaler = MinMaxScaler()\n","    scaled_dfs = []\n","    for df in dfs:\n","        scaled_data = scaler.fit_transform(df.values)\n","        scaled_dfs.append(pd.DataFrame(scaled_data, columns=df.columns))\n","    return scaled_dfs\n","\n","class TimeSeriesTransformer(nn.Module):\n","\n","    def __init__(self, input_dim, hidden_dim, latent_dim):\n","\n","        super().__init__()\n","        self.transformer = nn.TransformerEncoder(\n","            nn.TransformerEncoderLayer(d_model=input_dim, nhead=4),\n","            num_layers=4\n","        )\n","        self.mu_layer = nn.Linear(input_dim, latent_dim)\n","\n","        self.sigma_layer = nn.Linear(input_dim, latent_dim)\n","\n","    def forward(self, x_seq):\n","\n","        h = self.transformer(x_seq)\n","        mu = self.mu_layer(h.mean(dim=1))\n","        sigma = self.sigma_layer(h.mean(dim=1)).exp()\n","        return mu, sigma\n","\n","\n","# class TimeSeriesTransformer(nn.Module):\n","\n","#     def __init__(self, input_dim, hidden_dim=64, num_layers=3):\n","\n","#         super().__init__()\n","#         self.encoder = nn.Sequential(\n","#             nn.Linear(input_dim, hidden_dim),\n","#             nn.ReLU(),\n","#             *[nn.Sequential(\n","#                 nn.Linear(hidden_dim, hidden_dim),\n","#                 nn.ReLU()\n","#             ) for _ in range(num_layers-1)]\n","#         )\n","#         self.mu = nn.Linear(hidden_dim, hidden_dim)\n","#         self.logvar = nn.Linear(hidden_dim, hidden_dim)\n","\n","#     def forward(self, x):\n","\n","#         h = self.encoder(x)\n","#         return self.mu(h), self.logvar(h)\n","\n","\n","class Generator(nn.Module):\n","\n","    def __init__(self, noise_dim, cond_dim, output_dim):\n","\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(noise_dim + cond_dim, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, output_dim),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, z, c):\n","\n","        zc = torch.cat([z, c], dim=1)\n","        return self.net(zc)\n","\n","\n","class Discriminator(nn.Module):\n","\n","    def __init__(self, input_dim, cond_dim):\n","\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim + cond_dim, 512),\n","            nn.LeakyReLU(0.2),\n","            nn.Linear(512, 256),\n","            nn.LeakyReLU(0.2),\n","            nn.Linear(256, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x, c):\n","\n","        xc = torch.cat([x, c], dim=1)\n","        return self.net(xc)\n","\n","\n","def compute_mmd(x, y, sigma=1.0):\n","\n","    x_kernel = rbf_kernel(x, x, gamma=1.0/(2*sigma**2))\n","    y_kernel = rbf_kernel(y, y, gamma=1.0/(2*sigma**2))\n","    xy_kernel = rbf_kernel(x, y, gamma=1.0/(2*sigma**2))\n","\n","    mmd = x_kernel.mean() + y_kernel.mean() - 2*xy_kernel.mean()\n","    return mmd\n","\n","\n","def train_cgan(tab_dfs, time_df, num_epochs=1000, batch_size=32):\n","\n","    noise_dim = 64\n","    cond_dim = time_df.shape[1]\n","    feature_dim = tab_dfs[0].shape[1]\n","\n","    transformer = TimeSeriesTransformer(time_df.shape[1])\n","    generator = Generator(noise_dim, cond_dim, feature_dim)\n","    discriminator = Discriminator(feature_dim, cond_dim)\n","\n","    optim_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","    optim_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","    optim_t = optim.Adam(transformer.parameters(), lr=0.0001)\n","\n","    time_tensor = torch.FloatTensor(time_df.values)\n","    tab_tensors = [torch.FloatTensor(df.values) for df in tab_dfs]\n","\n","    all_tab_data = torch.cat(tab_tensors, dim=0)\n","    tab_loader = DataLoader(TensorDataset(all_tab_data), batch_size=batch_size, shuffle=True)\n","    time_loader = DataLoader(TensorDataset(time_tensor), batch_size=batch_size, shuffle=True)\n","\n","    feature_mmd_losses = {i: [] for i in range(feature_dim)}\n","\n","    for epoch in range(num_epochs):\n","\n","        for (tab_batch,), (time_batch,) in zip(tab_loader, time_loader):\n","\n","            current_batch_size = tab_batch.size(0)\n","            real_labels = torch.ones(current_batch_size, 1)\n","            fake_labels = torch.zeros(current_batch_size, 1)\n","\n","            mu, logvar = transformer(time_batch)\n","            cond = mu + torch.exp(0.5*logvar) * torch.randn_like(logvar)\n","            noise = torch.randn(current_batch_size, noise_dim)\n","            fake_data = generator(noise, cond)\n","            optim_d.zero_grad()\n","            real_loss = nn.BCELoss()(discriminator(tab_batch, cond), real_labels)\n","            fake_loss = nn.BCELoss()(discriminator(fake_data.detach(), cond), fake_labels)\n","            d_loss = (real_loss + fake_loss) / 2\n","\n","            d_loss.backward()\n","            optim_d.step()\n","            optim_g.zero_grad()\n","            optim_t.zero_grad()\n","\n","            noise = torch.randn(current_batch_size, noise_dim)\n","            fake_data = generator(noise, cond)\n","            g_loss_adv = nn.BCELoss()(discriminator(fake_data, cond), real_labels)\n","\n","            mmd_loss = 0\n","            for i in range(feature_dim):\n","\n","                tab_feature = tab_batch[:, i].view(-1, 1)\n","                fake_feature = fake_data[:, i].view(-1, 1)\n","                feature_mmd = compute_mmd(tab_feature.detach().numpy(), fake_feature.detach().numpy())\n","                mmd_loss += feature_mmd\n","                feature_mmd_losses[i].append(feature_mmd)\n","\n","            sampled_time = time_tensor[torch.randint(0, len(time_tensor), (current_batch_size,))]\n","            l2_loss = nn.MSELoss()(fake_data, sampled_time[:, :feature_dim])\n","            g_loss = g_loss_adv + mmd_loss + l2_loss\n","\n","            g_loss.backward()\n","            optim_g.step()\n","            optim_t.step()\n","\n","        print(f\"Epoch {epoch+1}/{num_epochs}, D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n","\n","    return generator, transformer, feature_mmd_losses\n","\n","\n","\n","def generate_and_build_graph(generator, transformer, time_df, tab_dfs, num_samples=1000):\n","\n","    noise = torch.randn(num_samples, 64)\n","    mu, logvar = transformer(torch.FloatTensor(time_df.values))\n","\n","    cond = mu + torch.exp(0.5*logvar) * torch.randn_like(logvar)\n","    cond = cond[torch.randint(0, len(cond), (num_samples,))]\n","    new_data = generator(noise, cond).detach().numpy()\n","    graphs = []\n","    for df in tab_dfs:\n","\n","        corr_matrix = df.corr().abs()\n","        threshold = 0.5\n","        adj_matrix = (corr_matrix > threshold).astype(int)\n","        G = nx.from_pandas_adjacency(adj_matrix)\n","        graphs.append(G)\n","    merged_graph = nx.Graph()\n","    for G in graphs:\n","\n","        merged_graph = nx.compose(merged_graph, G)\n","    avg_mmd_losses = {i: np.mean(losses) for i, losses in feature_mmd_losses.items()}\n","    min_mmd = min(avg_mmd_losses.values())\n","    max_mmd = max(avg_mmd_losses.values())\n","    for node in merged_graph.nodes():\n","\n","        feature_idx = int(node.split('_')[-1])\n","        mmd = avg_mmd_losses.get(feature_idx, max_mmd)\n","        weight = 1 - (mmd - min_mmd) / (max_mmd - min_mmd + 1e-8)\n","        merged_graph.nodes[node]['weight'] = weight\n","\n","    return new_data, merged_graph\n","\n","\n","def target_minmax_scale(source_series, target_series):\n","\n","    target_min = target_series.min()\n","    target_max = target_series.max()\n","    target_range = target_max - target_min\n","\n","    source_min = source_series.min()\n","    source_max = source_series.max()\n","    source_range = source_max - source_min\n","    if source_range == 0 or target_range == 0:\n","\n","        return source_series\n","\n","    scaled_series = (source_series - source_min) / source_range\n","    scaled_series = scaled_series * target_range + target_min\n","\n","    return scaled_series\n","\n","\n","def auto_encode_features(df, skip_columns=None, max_unique_for_label=20):\n","\n","    df = df.copy()\n","    skip_columns = skip_columns or []\n","\n","    for col in df.columns:\n","        if col in skip_columns:\n","\n","            continue\n","\n","        dtype = df[col].dtype\n","        if dtype == 'bool':\n","\n","            df[col] = df[col].astype(int)\n","        elif dtype == 'object' or isinstance(df[col].iloc[0], str):\n","\n","            num_unique = df[col].nunique()\n","            if 1 < num_unique <= max_unique_for_label:\n","\n","                df[col] = df[col].astype('category').cat.codes\n","            elif num_unique > max_unique_for_label:\n","\n","                dummies = pd.get_dummies(df[col], prefix=col)\n","                df = pd.concat([df, dummies], axis=1)\n","                df.drop(columns=[col], inplace=True)\n","\n","    return df\n","\n","\n","class LearnableGraph(nn.Module):\n","\n","    def __init__(self, num_nodes, hidden_dim):\n","\n","        super().__init__()\n","        self.num_nodes = num_nodes\n","        self.learnable_adj = nn.Parameter(torch.randn(num_nodes, num_nodes))\n","    def forward(self, x):\n","\n","        adj = torch.nn.functional.relu(self.learnable_adj)\n","        adj = torch.nn.functional.normalize(adj, p=1, dim=1)\n","        return adj\n","\n","class SelfAttention(nn.Module):\n","\n","    def __init__(self, hidden_dim):\n","\n","        super().__init__()\n","        self.att = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, x):\n","\n","        weights = F.softmax(self.att(x).squeeze(-1), dim=1)\n","        weighted = torch.bmm(weights.unsqueeze(1), x).squeeze(1)\n","        return weighted\n","\n","\n","# class GraphTimeModel(nn.Module):\n","#     def __init__(self, num_features, num_classes, num_nodes=20,\n","#                  hidden_dim=64, heads=4, static_edge_index=None):\n","#         super().__init__()\n","#         self.static_edge_index = static_edge_index\n","\n","#         self.graph_learner = LearnableGraph(num_nodes, hidden_dim)\n","\n","#         self.gat1 = GATConv(num_features, hidden_dim, heads=heads)\n","#         self.gat2 = GATConv(hidden_dim * heads, hidden_dim, heads=1)\n","\n","#         self.gru = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n","\n","#         self.classifier = nn.Linear(hidden_dim, num_classes)\n","\n","#     def forward(self, x_seq, edge_index_static):\n","#         B, T, N, F = x_seq.shape\n","#         device = x_seq.device\n","\n","#         outputs = []\n","\n","#         for t in range(T):\n","#             x_t = x_seq[:, t, :, :]\n","#             adj = self.graph_learner.learnable_adj\n","#             edge_index, edge_weight = dense_to_sparse(adj)\n","\n","#             edge_index_combined = torch.cat([edge_index_static.to(device), edge_index.to(device)], dim=1)\n","#             edge_weight_combined = None\n","\n","#             h = F.relu(self.gat1(x_t, edge_index_combined, edge_weight=edge_weight_combined))\n","#             h = self.gat2(h, edge_index_combined)\n","\n","#             outputs.append(h.mean(dim=1))\n","#         h_seq = torch.stack(outputs, dim=1)  # shape: (B, T, H)\n","#         out, _ = self.gru(h_seq)\n","#         logits = self.classifier(out[:, -1, :])\n","\n","#         return logits\n","\n","\n","class GraphTimeModel(nn.Module):\n","\n","    def __init__(self, num_features, num_classes, num_nodes=13,\n","                 hidden_dim=64, heads=4, static_edge_index=None):\n","\n","        super().__init__()\n","\n","        self.static_edge_index = static_edge_index\n","\n","        self.graph_learner = LearnableGraph(num_nodes, hidden_dim)\n","\n","        self.gcn1 = GATConv(num_features, hidden_dim)\n","        self.gcn2 = GATConv(hidden_dim, hidden_dim)\n","        self.gcn3 = GATConv(hidden_dim, hidden_dim)\n","        self.gcn4 = GATConv(hidden_dim, hidden_dim)\n","\n","        # self.gru = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n","\n","        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=4),\n","          num_layers=2)\n","\n","        self.classifier = nn.Linear(hidden_dim, num_classes)\n","\n","        # self.attention = SelfAttention(hidden_dim)\n","\n","    def forward(self, x_seq, edge_index_static):\n","\n","        B, T, N, F = x_seq.shape\n","        device = x_seq.device\n","\n","        outputs = []\n","\n","        for t in range(T):\n","\n","            x_t = x_seq[:, t, :, :]\n","            adj = self.graph_learner.learnable_adj\n","            edge_index, edge_weight = dense_to_sparse(adj)\n","            assert edge_index.max() < x_t.size(1), \\\n","                \"Edge index contains invalid node indices\"\n","            # edge_index_combined = torch.cat([edge_index_static.to(device), edge_index.to(device)], dim=1)\n","            # edge_weight_combined = None\n","\n","            alpha = 0.5\n","            edge_index_combined = torch.cat([edge_index, edge_index_static], dim=1)\n","            edge_weight_combined = torch.cat([\n","                alpha * edge_weight_dynamic,\n","                (1 - alpha) * edge_weight_static\n","            ])\n","            # h = F.relu(self.gat1(x_t, edge_index_combined, edge_attr=edge_weight_combined))\n","            h = torch.nn.functional.relu(self.gcn1(x_t, edge_index_combined, edge_weight=edge_weight_combined))\n","            h = self.gcn2(h, edge_index_combined, edge_weight=edge_weight_combined)\n","\n","            h = self.gcn3(h, edge_index_combined, edge_weight=edge_weight_combined)\n","            h = self.gcn4(h, edge_index_combined, edge_weight=edge_weight_combined)\n","\n","            outputs.append(h.mean(dim=1))\n","        h_seq = torch.stack(outputs, dim=1)\n","        out, _ = self.transformer(h_seq)\n","        logits = self.classifier(out[:, -1, :])\n","        return logits\n","\n","\n","def align_and_standardize_units(df_time, df_tab_aligned, similarity_threshold=0.6):\n","\n","    report = []\n","    processed_df = df_tab_aligned.copy()\n","    time_cols = df_time.columns.tolist()\n","\n","    common_cols = set(processed_df.columns) & set(time_cols)\n","\n","    for tab_col in df_tab_aligned.columns:\n","\n","        if tab_col in common_cols:\n","\n","            time_col = tab_col\n","            similarity = 1.0\n","        else:\n","            time_col, similarity = find_most_similar_time_col(tab_col, time_cols)\n","            if similarity < similarity_threshold:\n","\n","                report.append({\n","                    'original_column': tab_col,\n","                    'new_column': tab_col,\n","                    'similarity': similarity,\n","                    'action': 'No sufficiently similar time series column found',\n","                    'conversion': None\n","                })\n","                continue\n","\n","            processed_df = processed_df.rename(columns={tab_col: time_col})\n","            report.append({\n","                'original_column': tab_col,\n","                'new_column': time_col,\n","                'similarity': similarity,\n","                'action': 'Column renamed based on semantic similarity',\n","                'conversion': None\n","            })\n","        if not np.issubdtype(processed_df[time_col].dtype, np.number):\n","\n","            report.append({\n","                'column': time_col,\n","                'action': 'Non-numeric column - no unit conversion',\n","                'conversion': None\n","            })\n","            continue\n","\n","        time_unit = extract_unit_from_column(time_col)\n","        tab_unit = extract_unit_from_column(tab_col)\n","\n","        if time_unit and tab_unit:\n","\n","            if are_units_convertible(time_unit, tab_unit):\n","\n","                processed_df[time_col] = processed_df[time_col].apply(\n","                    lambda x: convert_units(x, tab_unit, time_unit)\n","                )\n","                report.append({\n","                    'column': time_col,\n","                    'action': 'Converted using explicit units',\n","                    'conversion': f'{tab_unit} → {time_unit}',\n","                    'factor': UNIT_CONVERSION[detect_unit_category(tab_unit)][tab_unit] /\n","                             UNIT_CONVERSION[detect_unit_category(time_unit)][time_unit]\n","                })\n","            else:\n","\n","                processed_df[time_col] = target_minmax_scale(\n","                    processed_df[time_col],\n","                    df_time[time_col]\n","                )\n","                report.append({\n","                    'column': time_col,\n","                    'action': 'Incompatible units - scaled to time series range',\n","                    'conversion': None\n","                })\n","\n","        elif time_unit and not tab_unit:\n","\n","            is_proportional, factor = statistical_unit_check(\n","                processed_df[time_col], df_time[time_col]\n","            )\n","\n","            if is_proportional:\n","\n","                processed_df[time_col] = processed_df[time_col] * factor\n","                report.append({\n","                    'column': time_col,\n","                    'action': 'Converted using statistical scaling',\n","                    'conversion': f'factor: {factor:.4f}'\n","                })\n","            else:\n","\n","                processed_df[time_col] = target_minmax_scale(\n","                    processed_df[time_col],\n","                    df_time[time_col]\n","                )\n","                report.append({\n","                    'column': time_col,\n","                    'action': 'Scaled to time series range',\n","                    'conversion': None\n","                })\n","        elif not time_unit and tab_unit:\n","\n","            report.append({\n","                'column': time_col,\n","                'action': 'Kept time series unitless format',\n","                'conversion': None\n","            })\n","\n","        else:\n","\n","            is_proportional, factor = statistical_unit_check(\n","                processed_df[time_col], df_time[time_col]\n","            )\n","\n","            if is_proportional and abs(factor - 1.0) > 0.01:\n","\n","                processed_df[time_col] = processed_df[time_col] * factor\n","                report.append({\n","                    'column': time_col,\n","                    'action': 'Adjusted by statistical scaling',\n","                    'conversion': f'factor: {factor:.4f}'\n","                })\n","            else:\n","\n","                processed_df[time_col] = target_minmax_scale(\n","                    processed_df[time_col],\n","                    df_time[time_col]\n","                )\n","                report.append({\n","                    'column': time_col,\n","                    'action': 'Scaled to time series range',\n","                    'conversion': None\n","                })\n","\n","    return processed_df, report\n","\n","\n","def preprocess_column_name(col_name):\n","\n","    return str(col_name).lower().replace('_', ' ').strip()\n","\n","def detect_antonym_relation(col1, col2):\n","\n","    col1 = preprocess_column_name(col1)\n","    col2 = preprocess_column_name(col2)\n","    result = classifier({\n","        'text': f\"The feature is {col1}\",\n","        'text_pair': f\"The feature is {col2}\",\n","    }, top_k=3)\n","    for item in result:\n","\n","        if item['label'] == 'contradiction':\n","            return (item['score'] > 0.7), item['score']\n","\n","    return False, 0.0\n","\n","\n","def is_boolean_column(series):\n","\n","    unique_vals = set(series.dropna().unique())\n","    return unique_vals.issubset({0, 1}) if unique_vals else False\n","\n","def align_column_names(df_time, df_tab_relevant, threshold=0.5):\n","\n","    time_cols = df_time.columns.tolist()\n","    tab_cols = df_tab_relevant.columns.tolist()\n","\n","    processed_time_cols = [preprocess_column_name(col) for col in time_cols]\n","    processed_tab_cols = [preprocess_column_name(col) for col in tab_cols]\n","\n","    time_embeddings = model.encode(processed_time_cols)\n","    tab_embeddings = model.encode(processed_tab_cols)\n","    similarity_matrix = cosine_similarity(tab_embeddings, time_embeddings)\n","\n","    report = {\n","        'column_mappings': [],\n","        'antonym_processed': []\n","    }\n","\n","    aligned_df = pd.DataFrame(index=df_tab_relevant.index)\n","\n","    for tab_idx, tab_col in enumerate(tab_cols):\n","\n","        time_idx = np.argmax(similarity_matrix[tab_idx])\n","        best_match_col = time_cols[time_idx]\n","        similarity_score = similarity_matrix[tab_idx][time_idx]\n","        is_antonym, antonym_confidence = detect_antonym_relation(tab_col, best_match_col)\n","\n","        if is_antonym:\n","\n","            if is_boolean_column(df_tab_relevant[tab_col]):\n","\n","                inverted_series = df_tab_relevant[tab_col].apply(lambda x: 1 if x == 0 else 0)\n","                aligned_df[best_match_col] = inverted_series\n","                report['antonym_processed'].append({\n","                    'original_column': tab_col,\n","                    'new_column': best_match_col,\n","                    'similarity': similarity_score,\n","                    'antonym_confidence': antonym_confidence,\n","                    'action': 'Boolean values inverted'\n","                })\n","            else:\n","\n","                aligned_df[tab_col] = df_tab_relevant[tab_col]\n","                report['antonym_processed'].append({\n","                    'original_column': tab_col,\n","                    'new_column': tab_col,\n","                    'similarity': similarity_score,\n","                    'antonym_confidence': antonym_confidence,\n","                    'action': 'Non-boolean column, only renamed'\n","                })\n","        else:\n","\n","            aligned_df[tab_col] = df_tab_relevant[tab_col]\n","            report['column_mappings'].append({\n","                'original_column': tab_col,\n","                'new_column': tab_col,\n","                'similarity': similarity_score,\n","                'action': 'Aligned by semantic similarity'\n","            })\n","\n","    return aligned_df, report\n","\n","\n","def prepare_data(df_time):\n","\n","    samples = []\n","    labels = []\n","\n","    for pid, group in df_time.groupby('Date'):\n","\n","        time_steps = sorted(group['timestep'].unique())\n","        seq = []\n","        for t in time_steps:\n","            data_t = group[group['timestep'] == t].drop(columns=['Date', 'timestep', 'Target'])\n","            seq.append(data_t.values)\n","\n","        sample = np.concatenate(seq, axis=0)\n","        label = group['Target'].iloc[-1]\n","        samples.append(sample)\n","        labels.append(label)\n","    X = np.stack(samples)\n","    y = np.array(labels)\n","\n","    return X, y\n","\n","def preprocess_column_names(columns):\n","\n","    processed = []\n","    for col in columns:\n","\n","        col = str(col).lower().replace('_', ' ').replace('-', ' ')\n","        col = ' '.join(col.split())\n","        processed.append(col)\n","    return processed\n","\n","def find_most_similar_columns(time_series_cols, table_cols, threshold=0.5):\n","\n","    processed_time_cols = preprocess_column_names(time_series_cols)\n","    processed_table_cols = preprocess_column_names(table_cols)\n","    time_embeddings = model.encode(processed_time_cols)\n","\n","    table_embeddings = model.encode(processed_table_cols)\n","    similarity_matrix = cosine_similarity(time_embeddings, table_embeddings)\n","\n","    results = {}\n","    for i, time_col in enumerate(time_series_cols):\n","\n","        similarities = similarity_matrix[i]\n","        max_idx = np.argmax(similarities)\n","        max_similarity = similarities[max_idx]\n","        if max_similarity > threshold:\n","            results[time_col] = (table_cols[max_idx], max_similarity)\n","\n","    return results\n","\n","\n","def get_most_relevant_features(df_time, df_tables, threshold=0.5):\n","\n","    time_cols = df_time.columns.tolist()\n","    if isinstance(df_tables, dict):\n","\n","        table_list = list(df_tables.values())\n","        table_names = list(df_tables.keys())\n","    else:\n","\n","        table_list = df_tables\n","\n","        table_names = [f'table_{i+1}' for i in range(len(table_list))]\n","    all_results = {}\n","    for df, name in zip(table_list, table_names):\n","\n","        table_cols = df.columns.tolist()\n","        matches = find_most_similar_columns(time_cols, table_cols, threshold)\n","        all_results[name] = matches\n","\n","    return all_results\n","\n","\n","def convert_to_timesteps(df, time_col='Time', patient_id_col='Date'):\n","\n","    df = df.copy()\n","    df[time_col] = pd.to_datetime(df[time_col])\n","    df['timestep'] = df.groupby(patient_id_col)[time_col].rank(method='first').astype(int) - 1\n","    df.drop(columns=[time_col], inplace=True)\n","\n","    return df\n","\n","\n","def select_relevant_columns(df_tab, matches):\n","\n","    relevant_cols = [match[0] for match in matches.values()]\n","    return df_tab[relevant_cols]\n","\n","\n","if __name__ == \"__main__\":\n","\n","\n","  df_time = pd.read_csv(\"df_time.csv\")\n","\n","  df_tab1 = pd.read_csv(\"df_tab1.csv\")\n","  df_tab2 = pd.read_csv(\"df_tab2.csv\")\n","  df_tab3 = pd.read_csv(\"df_tab3.csv\")\n","\n","  df_time = df_time.dropna()\n","  df_time = convert_to_timesteps(df_time)\n","\n","  model = SentenceTransformer('all-mpnet-base-v2')\n","\n","  classifier = pipeline(\"text-classification\", model=\"roberta-large-mnli\")\n","\n","  tables = {\n","      'table_1': df_tab1,\n","      'table_2': df_tab2,\n","      'table_3': df_tab3\n","  }\n","\n","  [df_tab1, df_tab2, df_tab3], [df_time_scaled] = preprocess_data(df_tab1_final, df_tab2_final, df_tab3_final, df_time)\n","\n","  generator, transformer, feature_mmd_losses = train_cgan([df_tab1, df_tab2, df_tab3], df_time_scaled)\n","\n","  relevant_features = get_most_relevant_features(df_time, tables)\n","\n","  df_tab1_relevant = select_relevant_columns(df_tab1, relevant_features['table_1'])\n","  df_tab2_relevant = select_relevant_columns(df_tab2, relevant_features['table_2'])\n","  df_tab3_relevant = select_relevant_columns(df_tab3, relevant_features['table_3'])\n","\n","  df_tab1_aligned, report1 = align_column_names(df_time, df_tab1_relevant)\n","  df_tab2_aligned, report2 = align_column_names(df_time, df_tab2_relevant)\n","  df_tab3_aligned, report3 = align_column_names(df_time, df_tab3_relevant)\n","\n","  df_tab1_final, report1 = align_and_standardize_units(df_time, df_tab1_aligned)\n","  df_tab2_final, report2 = align_and_standardize_units(df_time, df_tab2_aligned)\n","  df_tab3_final, report3 = align_and_standardize_units(df_time, df_tab3_aligned)\n","\n","  new_data, G_unified = generate_and_build_graph(generator, transformer, df_time_scaled, [df_tab1, df_tab2, df_tab3])\n","\n","  df_time = auto_encode_features(df_time, skip_columns=['Date'])\n","\n","  min_timesteps = df_time.groupby('Date')['timestep'].max().min()\n","\n","  df_time = df_time[df_time['timestep'] <= min_timesteps]\n","\n","  X, y = prepare_data(df_time)\n","\n","  X_train, X_test, y_train, y_test, len_train, len_test = train_test_split(X, y, lengths, test_size=0.2,random_state=42, stratify =y)\n","\n","  X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n","\n","  y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n","\n","  X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n","\n","  y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n","\n","  _, y_train_tensor = torch.unique(y_train_tensor, return_inverse=True)\n","\n","  _, y_test_tensor = torch.unique(y_test_tensor, return_inverse=True)\n","\n","  data_static = from_networkx(G_unified)\n","\n","  edge_index_static = data_static.edge_index\n","\n","  model = GraphTimeModel(\n","          num_features=1,\n","          num_classes=len(np.unique(y)),\n","          num_nodes=13,\n","          hidden_dim=32,\n","          heads=2,\n","          static_edge_index=edge_index_static)\n","\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","\n","  for epoch in range(2000):\n","\n","      model.train()\n","      optimizer.zero_grad()\n","\n","      x_input = X_train_tensor.unsqueeze(-1)\n","      output = model(x_input, edge_index_static)\n","      loss = criterion(output, y_train_tensor)\n","\n","      loss.backward()\n","      optimizer.step()\n","      if epoch % 10 == 0:\n","          print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n","\n","  model.eval()\n","\n","  with torch.no_grad():\n","\n","      x_input = X_test_tensor.unsqueeze(-1)\n","      logits = model(x_input, edge_index_static)\n","\n","      probs = F.softmax(logits, dim=1)\n","      pred = logits.argmax(dim=1)\n","      y_true = y_test_tensor.numpy()\n","      y_pred = pred.numpy()\n","\n","      y_probs = probs.numpy()\n","      f1 = f1_score(y_true, y_pred, average='weighted')\n","\n","      print(f\"F1 Score: {f1:.4f}\")\n","      if len(np.unique(y_true)) == 2:\n","\n","          auroc = roc_auc_score(y_true, y_probs[:, 1], average='micro')\n","          auprc = average_precision_score(y_true, y_probs[:, 1], average='micro')\n","          print(f\"AUROC: {auroc:.4f}\")\n","          print(f\"AUPRC: {auprc:.4f}\")\n","      else:\n","\n","          auroc = roc_auc_score(y_true, y_probs, multi_class='ovr', average='micro')\n","          auprc = average_precision_score(y_true, y_probs, average='micro')\n","          print(f\"AUROC: {auroc:.4f}\")\n","          print(f\"AUPRC: {auprc:.4f}\")"],"metadata":{"id":"kuAXJwfr_tOu"},"execution_count":null,"outputs":[]}]}